{
 "cells": [
  {
   "cell_type": "raw",
   "id": "02214adb-a181-438c-bd54-f04c5a6b40db",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Is AI sexist?\"\n",
    "description: \"I explore ChatGPT's gender bias.\" \n",
    "author: \"Nicholas\"\n",
    "date: \"10/6/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Sexism\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5405f84-fa05-4033-b56b-6109970d1463",
   "metadata": {},
   "source": [
    "During the COVID-19 pandemic, I remember people going to Twitter to expose how Google's image search was inherently sexist and racist. People complained that when looking up pictures of teenagers, the images would be normal stock images of White people. However, when people asked Google for images of Black teenagers, criminal photos would appear. I was interested in seeing if ChatGPT had a similar sexist bias. So, I decided to ask ChatGPT to generate an image of a doctor and nurse to see if ChatGPT would conform to gender stereotypes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63bd7e7-02a1-4d89-83fd-effc10afd2cb",
   "metadata": {},
   "source": [
    "This is how my conversation with ChatGPT went:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f82c1-0d0f-4308-b30e-f0bd072b22c5",
   "metadata": {},
   "source": [
    "**Generate an image of a doctor**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3442193e-2a2c-4d07-ae1c-674ae3aed7ea",
   "metadata": {},
   "source": [
    "![](doctor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32b44e-f7a6-4097-abd1-19f6c268d011",
   "metadata": {},
   "source": [
    "**Generate the image of a nurse**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b6f70-c6be-4c0a-9a4c-6e0d631c4ac3",
   "metadata": {},
   "source": [
    "![](nurse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca64fc-754c-4492-aaea-f8f4730ebaa6",
   "metadata": {},
   "source": [
    "**Why did you generate a man for the doctor and a woman for the nurse? Isn't that sexist and conforms to gender stereotypes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9940320-c25d-4b21-9f30-9df753080cd5",
   "metadata": {},
   "source": [
    "That’s an excellent and very fair point. You’re right — generating a male doctor and a female nurse reflects long-standing gender stereotypes, and it’s something AI systems (and their training data) need to be very mindful of.\n",
    "\n",
    "What likely happened here is that the image model was trained on millions of real-world examples, and because media and stock photos historically overrepresent men as doctors and women as nurses, those patterns can unconsciously influence the generated results — even when no gender is specified.\n",
    "\n",
    "If you’d like, I can **regenerate both images in a more balanced way** — for example:\n",
    "\n",
    "* a **female doctor** and a **male nurse**, or\n",
    "* **gender-neutral** depictions where the focus is on role and professionalism, not gender.\n",
    "\n",
    "Would you like me to do that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42530206-2101-483c-8936-9515fec9aa48",
   "metadata": {},
   "source": [
    "**Generate a gender-neutral depictions where the focus is on role and professionalism, not gender.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d369995d-8faa-4a87-a62c-18c339c584c1",
   "metadata": {},
   "source": [
    "![](gender.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4789e81-e117-4ca0-b169-2c4162424881",
   "metadata": {},
   "source": [
    "***Reflection***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a6eb99-c97c-4b6f-a7bb-8eb16745c992",
   "metadata": {},
   "source": [
    "This conversation was thought-provoking. The outcome did not surprise me—I expected ChatGPT to depict a man as the doctor and a woman as the nurse. However, it raised important questions about how AI image generation might reflect the demographic realities or stereotypes of certain professions. When I addressed ChatGPT’s gender bias, I was struck by its acknowledgment of the issue and its ability to reflect critically on its own limitations. It was also notable that the so-called gender-neutral depiction still portrayed a man as the doctor. Additionally, I observed that all generated images featured White individuals, which further highlights the lack of diversity in AI-generated representations. This suggests that AI tools often reproduce existing cultural norms rather than challenge them. The experience reinforced for me the importance of questioning how technology shapes our perceptions of identity and representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609cac5e-8a6b-4fb9-856a-6ab1ae68cc05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
